{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Libraries"
      ],
      "metadata": {
        "id": "1zZOq4Yz2_hz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In order to perform data preprocessing using Python, we need to import some predefined Python libraries. These libraries are used to perform some specific jobs. here are three specific libraries that we will use for data preprocessing, which are: Numpy, Pandas and Matplotlib."
      ],
      "metadata": {
        "id": "G7-FMVj2E_ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "d-5LUM6T3AVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Dataset"
      ],
      "metadata": {
        "id": "G0btTJF23ENP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Now, we need to import the dataset which we have collected for our classification project. In order to import the dataset, we will use read_csv() function of pandas library, which is used to read a csv file and performs various operations on it."
      ],
      "metadata": {
        "id": "HvKU3Q2PFPDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"iris.data.csv\", header = None)\n",
        "dataset.columns = [\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\", \"Species\"]\n",
        "X = dataset.iloc[:,:4].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "36ETjU643CEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.shape)"
      ],
      "metadata": {
        "id": "TU-nVdxF3GIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.head())"
      ],
      "metadata": {
        "id": "HCwyTBWp3OMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.info())"
      ],
      "metadata": {
        "id": "E2T-EGiT3OPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.describe())"
      ],
      "metadata": {
        "id": "w1DDomER3OR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.nunique())"
      ],
      "metadata": {
        "id": "iFAka-ZE3OWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"Species\"].unique())"
      ],
      "metadata": {
        "id": "R4mFbmOY3T9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "dcXvp11C3dtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding Missing Data"
      ],
      "metadata": {
        "id": "cgx9xwgt3iAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The next step of data preprocessing is to handle missing data in the dataset. However, in this case we find that there are no missing values in the dataset."
      ],
      "metadata": {
        "id": "4yNQuOMfFzXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.isna().sum())"
      ],
      "metadata": {
        "id": "YwZHMiGS3Y59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding Categorical Data - Dependent Variable"
      ],
      "metadata": {
        "id": "N5BDAsau3lwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Here, the categorical variable is \"Species\". Since Machine Learning models are primarily based on mathematical equations, we need to encode the categorical variable and the technique used here is Label Encoding - Converting categorical columns into numerical ones."
      ],
      "metadata": {
        "id": "XmftBPgiGUNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "h3upoo-J3jpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ],
      "metadata": {
        "id": "m4YCRBvn33l1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Splitting the dataset is the next step in data preprocessing in machine learning. Every dataset must be split into two separate sets â€“ training set (we already know the output) and test set (model predicts the output)."
      ],
      "metadata": {
        "id": "WRl4Qf_eHc5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 23)"
      ],
      "metadata": {
        "id": "w3-f0FfP3npL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling"
      ],
      "metadata": {
        "id": "hq8KOB9J38XR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Feature scaling is the final step of data preprocessing, where we standardize the independent variables of the dataset in a specific range. In feature scaling, we put our variables in the same range and in the same scale so that no any variable dominate the other variable. Here, we use the Standardization method for our dataset."
      ],
      "metadata": {
        "id": "BMCrw1dHHlzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "OJlTkouI34t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "wBZ4mO6e4Adg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Principal Component Analysis is done to reduce the number of dimensions in the training dataset and de-noise the data, thereby employing Feature Extraction."
      ],
      "metadata": {
        "id": "zdyUf9nxIg4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 4)\n",
        "temp = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "_FMqBcaE39wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PC_values = np.arange(pca.n_components_) + 1\n",
        "plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eDHk-Lxw4Cav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "Cbx8NU4B4DZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "MJ-LaAeI4Eg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "GbzrCSE24HLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train_pca, y_train)"
      ],
      "metadata": {
        "id": "pCXZILAa4FiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(X_test_pca)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "sLDTBKy-4JKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Evaluation"
      ],
      "metadata": {
        "id": "_P82rQKRPwYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Since accuracy can be very misleading as it does not take class imbalance into account, we look into other Multiclass Classification metrics like:\n",
        "\n",
        "1. Cohenâ€™s Kappa score\n",
        "2. Matthewâ€™s correlation coefficient\n",
        "3. Classification report\n",
        "\n",
        "##### All of the metrics used are associated with confusion matrices in one way or the other."
      ],
      "metadata": {
        "id": "ENxOXfCGP5Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score, matthews_corrcoef, classification_report\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred),3))\n",
        "print(\"Cohenâ€™s Kappa score:\", round(cohen_kappa_score(y_test, y_pred),3))\n",
        "print(\"Matthewâ€™s correlation coefficient:\", round(matthews_corrcoef(y_test, y_pred),3))\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred, target_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']))"
      ],
      "metadata": {
        "id": "TxWRkF8f4KFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, cmap = 'summer')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "163R5sa888Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_train_pca, y_train\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                color = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
        "plt.title('Logistic Regression (Training set)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CairZtgt4Pst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = X_test_pca, y_test\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                color = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
        "plt.title('Logistic Regression (Test set)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VsB58Ie28Md0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN - Artificial Neural Network"
      ],
      "metadata": {
        "id": "_Iq9vy-d8OyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.utils import np_utils"
      ],
      "metadata": {
        "id": "-i1YIrHJ8N26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Changing the label to One Hot Vector using One Hot Encoding - Represent categorical variables as numerical values"
      ],
      "metadata": {
        "id": "-EZVveFgN0QQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np_utils.to_categorical(y_train, num_classes = 3)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes = 3)"
      ],
      "metadata": {
        "id": "KJChfYcFAJ3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "id": "djugfPmlOKn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(units = 1000, input_dim = 4, activation = 'relu'))\n",
        "model.add(Dense(units = 500, activation = 'relu'))\n",
        "model.add(Dense(units = 300, activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units = 3, activation = 'softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "9FbWo5Q_-480"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "fH19vdbmAa1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, validation_data = (X_test, y_test), batch_size = 20, epochs = 10, verbose = 1)"
      ],
      "metadata": {
        "id": "WFY4lFmw_Gcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_test = np.argmax(y_test, axis = 1)\n",
        "y_pred = np.argmax(y_pred, axis = 1)"
      ],
      "metadata": {
        "id": "FhhXlw1N_IEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "id": "KTUKSRhlMPGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Evaluation"
      ],
      "metadata": {
        "id": "1wIfF9XdP2bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score, matthews_corrcoef, classification_report\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred),3))\n",
        "print(\"Cohenâ€™s Kappa score:\", round(cohen_kappa_score(y_test, y_pred),3))\n",
        "print(\"Matthewâ€™s correlation coefficient:\", round(matthews_corrcoef(y_test, y_pred),3))\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred, target_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']))"
      ],
      "metadata": {
        "id": "urALtXrhAiXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, cmap = 'summer')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XzWN79XCEbHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}